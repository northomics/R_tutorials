---
title: "Stats in R"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
knitr::opts_chunk$set(echo = FALSE)
```


## Linear regression

Now that we've got the hang of some R functions, let's do some real analysis!

### What is linear regression? 

- explains the linear relationship between dependent (Y) and independent (X) variables 
- "line of best fit" to data
- a "simple" linear regression is equivalent to a *correlation*
- can be considered *supervised* machine learning technique ([see here](http://machinelearningmastery.com/linear-regression-for-machine-learning/))
    - the model learns from known data, and can be used to predict
    - sometimes we are looking for trends and don't care about predictions

linear regression takes the linear equation of:

$$ Y = mX + b $$
But we're going to use it more like:

$$ \hat{y_{i}} = \hat{\beta_{0}} + \hat{\beta_{1}}x_{i} + \epsilon_{i}$$

where:  
$\hat{y_{i}} =$ predicted response for an experimental unit $i$  
$x_{i} =$ predictor (or independent variable) of experimental unit $i$  
$\hat{\beta_{0}} =$ is expected value when $x_{i} = 0$ (intercept)  
$\hat{\beta_{1}} =$ slope  
$\epsilon =$ some error, because models are never perfect!  

[//]: $\hat{\beta_{0}}$ and $\hat{\beta_{1}}$ are unknown


[//]: **$\beta_{0}$ and $\beta_{1}$: what we have to estimate**

$\beta_{0}$ and $\beta_{1}$ are unknown. We will *estimate these coefficients* from known data.
To do this, we need to estimate a line of best fit than minimizes error... this is where *linear regression* comes in!

## Load some data!

We have options for loading data. 

1. We read use `read.csv()` to read in data from a file.  
2. We can use a built-in dataset from an R package.

Today we'll use a built in dataset called the [palmerpenguins](https://allisonhorst.github.io/palmerpenguins/index.html) dataset. 


```{r penguin, echo = FALSE, out.width = "100%", fig.cap = "Artwork by @allison_horst."}
knitr::include_graphics("images/penguins.png")
```



```{r dataloading, exercise=TRUE}
# install the package, normally required
# install.packages("palmerpenguins")
# load the package
library(palmerpenguins)
# call the data from the package
data(package = 'palmerpenguins') 
head(penguins)
```

We will focus on two penguin measurements for today:

```{r penguinmeasures, echo = FALSE, out.width = "100%", fig.cap = "Artwork by @allison_horst."}
knitr::include_graphics("images/measurements.png")
```


## Linear regression with data

Ok, let's look at the data to see if a linear model looks reasonable...


### Simple visual check

```{r penguincor, exercise=TRUE}
plot(penguins$bill_length_mm, penguins$bill_depth_mm, 
     main="Bill length vs bill depth", xlab = "Bill length (mm)", ylab = "
     Bill depth (mm)")
```

Hmmm... not sure yet. But! Remember! There are **three** types of penguins in the data. Maybe we'll see some patterns later on...

## Setting up a linear regression

Let's estimate the coefficients we were talking about earlier.

- `lm()` is the linear model function. 
- `formula` and `data` are parameters

Try using this code. What does `summary()` give us? What does `abline()` do?

```{r linearmodel, exercise=TRUE}
lm1 <- lm(formula = bill_length_mm ~ bill_depth_mm, data = penguins)
summary(lm1)
plot(penguins$bill_length_mm, penguins$bill_depth_mm, 
     main="Bill length vs bill depth", xlab = "Bill length (mm)", ylab = "
     Bill depth (mm)")
abline(lm1, col="purple")
```

### Summary output

Our summary output gives us a lot of information:

1. Info about distribution of residuals (errors)
2. The estimates of our coefficients
3. Standard error of coefficient estimates
    - square root of the variance 
      - $\sqrt{\sigma}^{2}$
3. t-values (coefficient estimates/standard error)
3. $R^{2}$ = (want closer to 1)
4. p-values 
    - testing if your coefficient = 0


```{r quizlm}
quiz(
  question("Look back at the summary. Does it seem like our linear model is a good fit to our data?",
    answer("Yes!! Look at the coefficient p-value!"),
    answer("Nope. Check out the line of best fit and the $R^{2}$", correct=T)
  )
)
```


## Linear models with more coefficients

OK, so maybe all penguins don't follow the same bill length to depth patterns. How about we test if the type of penguin has an effect on this pattern? We can add this information to our linear model using a different `formula=` parameter, but we have to formulate it on our own. 

We have some options on possible formulas for the linear regression:

* length ~ depth + species 
    * $\hat{length_{i}} = \hat{\beta_{0}} + \hat{\beta_{1}}depth_{i} + \hat{\beta_{2}}species_{i} + \epsilon_{i}$ 
* length ~ depth:species  
    * $\hat{length_{i}} = \hat{\beta_{0}} + \hat{\beta_{2}}species_{i}depth_{i} + \epsilon_{i}$ 
* length ~ depth*species = length ~ depth + species + depth x species 
    * $\hat{length_{i}} = \hat{\beta_{0}} + \hat{\beta_{1}}depth_{i} + \hat{\beta_{2}}species_{i} + \hat{\beta_{2}}species_{i}depth_{i} + \epsilon_{i}$ 

## Your first exercise    
 
OK, so we can run all of these functions, but how do we know what the output is? How can we look at coefficients, error estimate, etc?

Here's your chance to show off! Type how you would print out this information we need and then click on Run Code.

```{r lm_coefficients, exercise=TRUE, exercise_eval=T}
lm2 <- lm(formula = bill_length_mm ~ bill_depth_mm + species, data = penguins)
lm3 <- lm(formula = bill_length_mm ~ bill_depth_mm:species, data = penguins)
lm4 <- lm(formula =bill_length_mm ~ bill_depth_mm*species, data = penguins)
```

```{r lm_coefficients-solution}
lm2 <- lm(formula = bill_length_mm ~ bill_depth_mm + species, data = penguins)
lm3 <- lm(formula = bill_length_mm ~ bill_depth_mm:species, data = penguins)
lm4 <- lm(formula = bill_length_mm ~ bill_depth_mm*species, data = penguins)

summary(lm2)
summary(lm3)
summary(lm4)
```

How can we interpret these linear models? 

*Let's discuss*

## Visualizing more complex LMs

`ggplot2` is a great package for visualizing all types of data. I think it's more useful to learn how to use `ggplot2` than the "built-in" plotting functions in R. In my opinion, it is more intuitive.... but that's all up for debate. 

We're going to learn how to use ``ggplot2` to plot our penguin linear models! 

```{r ggplot-intro, exercise=T}
library(ggplot2)
```




```{r ggplotpenguin, exercise=T}
library(ggplot2)
(bill_len_dep <- ggplot(data = penguins,
                         aes(x = bill_length_mm,
                             y = bill_depth_mm,
                             group = species)) +
  geom_point(aes(color = species, 
                 shape = species),
             size = 3,
             alpha = 0.8) +
  geom_smooth(method = "lm", se = FALSE, aes(color = species)) +
  theme_minimal() +
  scale_color_manual(values = c("darkorange","purple","cyan4")) +
  labs(x = "Bill length (mm)",
       y = "Bill depth (mm)",
       color = "Penguin species",
       shape = "Penguin species") +
  theme(legend.position = c(0.85, 0.15),
        legend.background = element_rect(fill = "white", color = NA),
        plot.title.position = "plot",
        plot.caption = element_text(hjust = 0, face= "italic"),
        plot.caption.position = "plot"))

```